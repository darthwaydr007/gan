{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Imbalanced_MNIST.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darthwaydr007/gan/blob/master/Imbalanced_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNe282xiedwV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f382287e-cc6c-4d1a-db87-4e65ecc88b17"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import random\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU available.' % torch.cuda.device_count())\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU')\n",
        "    device = torch.device(\"cpu\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU available.\n",
            "GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhgEzVnIeiFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_train = pd.read_csv('/content/sample_data/mnist_train_small.csv')\n",
        "test_mnist = pd.read_csv('/content/sample_data/mnist_test.csv')\n",
        "mnist_train.rename(columns={'6':'label'}, \n",
        "                 inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOnS6gWPil9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = mnist_train[['label']]\n",
        "classes = train_labels['label'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBizs8GDeu-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imbalanced_mnist = pd.DataFrame()\n",
        "for i in classes:\n",
        "  temp = mnist_train[mnist_train['label'] == i]\n",
        "  if i%2 == 0:\n",
        "    temp = temp[:200]\n",
        "  else:\n",
        "    temp = temp[:1500]\n",
        "  imbalanced_mnist = pd.concat([imbalanced_mnist , temp])\n",
        "imbalanced_mnist = imbalanced_mnist.sample(frac=1).reset_index(drop=True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9A7H8Sreyjf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "balanced_mnist = pd.DataFrame()\n",
        "for i in classes:\n",
        "  temp = mnist_train[mnist_train['label'] == i]\n",
        "  temp = temp[:1500]\n",
        "  balanced_mnist = pd.concat([balanced_mnist , temp])\n",
        "balanced_mnist = balanced_mnist.sample(frac=1).reset_index(drop=True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6tOqi4VezS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_dataset(imbalanced_mnist):\n",
        "  imbalanced_mnist = imbalanced_mnist.sample(frac=1).reset_index(drop=True)\n",
        "  labels = imbalanced_mnist.iloc[:,0]\n",
        "  imbalanced_mnist.drop(imbalanced_mnist.columns[0], axis=1, inplace=True)\n",
        "  return imbalanced_mnist.values , labels.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTHezGIPe3p-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNISTDataset(Dataset):\n",
        "  def __init__(self , dataset):\n",
        "    self.data = dataset\n",
        "    self.dataset ,  self.labels = split_dataset(self.data)\n",
        "    self.dataset = self.dataset\n",
        "    self.labels = self.labels \n",
        "    self.length = len(self.labels)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "  def __getitem__(self , index):\n",
        "    return self.dataset[index] ,self.labels[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv4lr0Iae_Bc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 48\n",
        "imbalanced = MNISTDataset(imbalanced_mnist)\n",
        "balances = MNISTDataset(balanced_mnist)\n",
        "test  = MNISTDataset(test_mnist)\n",
        "train_loader_imbalanced = torch.utils.data.DataLoader(imbalanced , batch_size = BATCH_SIZE, shuffle = True , drop_last=True)\n",
        "train_loader_balanced = torch.utils.data.DataLoader(balances , batch_size = BATCH_SIZE, shuffle = True , drop_last=True)\n",
        "test_loader = torch.utils.data.DataLoader(test , batch_size = BATCH_SIZE, shuffle = True , drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz8Sq2gafTvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class mnistclassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model = nn.Sequential(\n",
        "            nn.Linear(784, 1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 10),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "  \n",
        "  def forward(self, x):\n",
        "        x = self.model(x.view(x.size(0),784))\n",
        "        return x.cuda()\n",
        "        #return F.log_softmax(x).cuda()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f45i3IaEfb6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.0002\n",
        "EPOCHS = 10\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "classifier1 = mnistclassifier().to(device)\n",
        "optimizer1 = torch.optim.Adam(classifier1.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "classifier2 = mnistclassifier().to(device)\n",
        "optimizer2 = torch.optim.Adam(classifier2.parameters(), lr=1e-4, weight_decay=1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzJPS_fchECr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "outputId": "7b69d0b4-4cb0-4212-8b42-bdb2005e50ca"
      },
      "source": [
        "for i in range(0,EPOCHS):\n",
        "  classifier1.train()\n",
        "  total_loss = 0\n",
        "  print('======== Epoch {:} / {:} ========'.format(i + 1, EPOCHS))\n",
        "  for batch , (img1 , label) in enumerate(train_loader_imbalanced):\n",
        "    \n",
        "    img1 = img1.cuda()\n",
        "    label = label.cuda()\n",
        "    \n",
        "    #print(label[0])\n",
        "    \n",
        "    optimizer1.zero_grad()\n",
        "    output = classifier1(img1.float())\n",
        "\n",
        "    #print(output[0])\n",
        "\n",
        "    #loss = F.nll_loss(output , label)\n",
        "    loss = criterion(output , label)\n",
        "    #print(loss)\n",
        "    loss.backward()\n",
        "    optimizer1.step()\n",
        "    \n",
        "    total_loss = total_loss + loss.mean().item()\n",
        "    if batch % 100 == 0 and not batch == 0:\n",
        "          print('AvgLoss : {:} , Batch Loss : {:}'.format(total_loss/batch , loss.mean().item()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======== Epoch 1 / 10 ========\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:100: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AvgLoss : 1.8300137722492218 , Batch Loss : 1.7580076456069946\n",
            "======== Epoch 2 / 10 ========\n",
            "AvgLoss : 1.658024297952652 , Batch Loss : 1.6093164682388306\n",
            "======== Epoch 3 / 10 ========\n",
            "AvgLoss : 1.6450414729118348 , Batch Loss : 1.6094976663589478\n",
            "======== Epoch 4 / 10 ========\n",
            "AvgLoss : 1.6414422500133514 , Batch Loss : 1.6358424425125122\n",
            "======== Epoch 5 / 10 ========\n",
            "AvgLoss : 1.6078879964351653 , Batch Loss : 1.6152281761169434\n",
            "======== Epoch 6 / 10 ========\n",
            "AvgLoss : 1.5830516624450683 , Batch Loss : 1.5435107946395874\n",
            "======== Epoch 7 / 10 ========\n",
            "AvgLoss : 1.5590054833889007 , Batch Loss : 1.5992933511734009\n",
            "======== Epoch 8 / 10 ========\n",
            "AvgLoss : 1.5497333753108977 , Batch Loss : 1.51008939743042\n",
            "======== Epoch 9 / 10 ========\n",
            "AvgLoss : 1.547432643175125 , Batch Loss : 1.5249286890029907\n",
            "======== Epoch 10 / 10 ========\n",
            "AvgLoss : 1.543432344198227 , Batch Loss : 1.5687576532363892\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR3emBWBhTTS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "bf6ecc1b-d1e3-400c-f734-ede64728b14f"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "odd_correct = 0\n",
        "odd_total = 0\n",
        "even_correct = 0\n",
        "even_total = 0\n",
        "classifier1.eval()\n",
        "with torch.no_grad():\n",
        "  for batch_idx, (img1 , label) in enumerate(test_loader):\n",
        "\n",
        "    img1 = img1.cuda()\n",
        "    #img1 = transform(img1.float().cpu().detach().numpy())\n",
        "    #img1 = img1.view(BATCH_SIZE,784).cuda()\n",
        "    #img1.requires_grad = True\n",
        "    label = label.cuda()\n",
        "    \n",
        "    output = classifier1(img1.float())\n",
        "\n",
        "    for idx ,i in enumerate(output):\n",
        "      #print(torch.argmax(i))\n",
        "      if idx%2 == 0:\n",
        "        if torch.argmax(i) == label[idx]:\n",
        "          even_correct += 1\n",
        "        even_total +=1\n",
        "      else:\n",
        "        if torch.argmax(i) == label[idx]:\n",
        "          odd_correct += 1\n",
        "        odd_total +=1\n",
        "      if torch.argmax(i) == label[idx]:\n",
        "        correct += 1\n",
        "      total += 1\n",
        "print(\"accuracy : \" , round(correct/total , 3))\n",
        "print(\"even accuracy : \" , round(even_correct/even_total , 3))\n",
        "print(\"odd accuracy : \" , round(odd_correct/odd_total , 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:100: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy :  0.836\n",
            "even accuracy :  0.834\n",
            "odd accuracy :  0.837\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueS8p581hcdf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "fe7bb9d8-23b1-4882-eeaa-25e721812e5f"
      },
      "source": [
        "for i in range(0,EPOCHS):\n",
        "  classifier2.train()\n",
        "  total_loss = 0\n",
        "  print('======== Epoch {:} / {:} ========'.format(i + 1, EPOCHS))\n",
        "  for batch , (img1 , label) in enumerate(train_loader_balanced):\n",
        "    \n",
        "    img1 = img1.cuda()\n",
        "    label = label.cuda()\n",
        "    \n",
        "    #print(label[0])\n",
        "    \n",
        "    optimizer2.zero_grad()\n",
        "    output = classifier2(img1.float())\n",
        "\n",
        "    #print(output[0])\n",
        "\n",
        "    #loss = F.nll_loss(output , label)\n",
        "    loss = criterion(output , label)\n",
        "    #print(loss)\n",
        "    loss.backward()\n",
        "    optimizer2.step()\n",
        "    \n",
        "    total_loss = total_loss + loss.mean().item()\n",
        "    if batch % 100 == 0 and not batch == 0:\n",
        "          print('AvgLoss : {:} , Batch Loss : {:}'.format(total_loss/batch , loss.mean().item()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======== Epoch 1 / 10 ========\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:100: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AvgLoss : 1.9629097282886505 , Batch Loss : 1.685844898223877\n",
            "AvgLoss : 1.8126718610525132 , Batch Loss : 1.6506977081298828\n",
            "AvgLoss : 1.7450712168216704 , Batch Loss : 1.6697584390640259\n",
            "======== Epoch 2 / 10 ========\n",
            "AvgLoss : 1.6003436291217803 , Batch Loss : 1.6080440282821655\n",
            "AvgLoss : 1.5870069307088852 , Batch Loss : 1.5826722383499146\n",
            "AvgLoss : 1.5780654199918112 , Batch Loss : 1.5251227617263794\n",
            "======== Epoch 3 / 10 ========\n",
            "AvgLoss : 1.5636235570907593 , Batch Loss : 1.5825849771499634\n",
            "AvgLoss : 1.555616604089737 , Batch Loss : 1.5465222597122192\n",
            "AvgLoss : 1.5520868241786956 , Batch Loss : 1.4644603729248047\n",
            "======== Epoch 4 / 10 ========\n",
            "AvgLoss : 1.551228187084198 , Batch Loss : 1.6008529663085938\n",
            "AvgLoss : 1.5393558859825134 , Batch Loss : 1.543160080909729\n",
            "AvgLoss : 1.5387337768077851 , Batch Loss : 1.5041661262512207\n",
            "======== Epoch 5 / 10 ========\n",
            "AvgLoss : 1.54174156665802 , Batch Loss : 1.5249437093734741\n",
            "AvgLoss : 1.5323598498106004 , Batch Loss : 1.486217975616455\n",
            "AvgLoss : 1.5316112192471822 , Batch Loss : 1.536866307258606\n",
            "======== Epoch 6 / 10 ========\n",
            "AvgLoss : 1.5356787979602813 , Batch Loss : 1.5248562097549438\n",
            "AvgLoss : 1.5278944379091264 , Batch Loss : 1.5179210901260376\n",
            "AvgLoss : 1.524863032102585 , Batch Loss : 1.5257624387741089\n",
            "======== Epoch 7 / 10 ========\n",
            "AvgLoss : 1.532647066116333 , Batch Loss : 1.5431431531906128\n",
            "AvgLoss : 1.5244131577014923 , Batch Loss : 1.5308903455734253\n",
            "AvgLoss : 1.519964269399643 , Batch Loss : 1.4828076362609863\n",
            "======== Epoch 8 / 10 ========\n",
            "AvgLoss : 1.5286540472507477 , Batch Loss : 1.4853705167770386\n",
            "AvgLoss : 1.5216313815116882 , Batch Loss : 1.4882025718688965\n",
            "AvgLoss : 1.5180779306093852 , Batch Loss : 1.523271083831787\n",
            "======== Epoch 9 / 10 ========\n",
            "AvgLoss : 1.5232459127902984 , Batch Loss : 1.5065840482711792\n",
            "AvgLoss : 1.5178427255153657 , Batch Loss : 1.5492730140686035\n",
            "AvgLoss : 1.512327956755956 , Batch Loss : 1.4899035692214966\n",
            "======== Epoch 10 / 10 ========\n",
            "AvgLoss : 1.5152909243106842 , Batch Loss : 1.5060954093933105\n",
            "AvgLoss : 1.5124338883161546 , Batch Loss : 1.5311299562454224\n",
            "AvgLoss : 1.509691598812739 , Batch Loss : 1.4949883222579956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bykAdezghivQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c1f28258-3197-4531-d2eb-ac7d111f9c64"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "odd_correct = 0\n",
        "odd_total = 0\n",
        "even_correct = 0\n",
        "even_total = 0\n",
        "classifier2.eval()\n",
        "with torch.no_grad():\n",
        "  for batch_idx, (img1 , label) in enumerate(test_loader):\n",
        "\n",
        "    img1 = img1.cuda()\n",
        "    #img1 = transform(img1.float().cpu().detach().numpy())\n",
        "    #img1 = img1.view(BATCH_SIZE,784).cuda()\n",
        "    #img1.requires_grad = True\n",
        "    label = label.cuda()\n",
        "    \n",
        "    output = classifier2(img1.float())\n",
        "\n",
        "    for idx ,i in enumerate(output):\n",
        "      #print(torch.argmax(i))\n",
        "      if idx%2 == 0:\n",
        "        if torch.argmax(i) == label[idx]:\n",
        "          even_correct += 1\n",
        "        even_total +=1\n",
        "      else:\n",
        "        if torch.argmax(i) == label[idx]:\n",
        "          odd_correct += 1\n",
        "        odd_total +=1\n",
        "      if torch.argmax(i) == label[idx]:\n",
        "        correct += 1\n",
        "      total += 1\n",
        "print(\"accuracy : \" , round(correct/total , 3))\n",
        "print(\"even accuracy : \" , round(even_correct/even_total , 3))\n",
        "print(\"odd accuracy : \" , round(odd_correct/odd_total , 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:100: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy :  0.948\n",
            "even accuracy :  0.948\n",
            "odd accuracy :  0.948\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9BB3GvXjeHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}